{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Neural Network Model Building\n",
    "This Jupyter Notebook is the product of Thomas Hymel. The contents pertain to one part of a Automatic Drum Transcription (ADT) project that I am working on to improve my data science and machine learning skills. This notebook in particular focuses on the neural network model building using Keras and TensorFlow. The data used for training the model will initially be the 25 song data set that has been created in other Jupyter Notebooks. The training data is in two matrices X (input) and Y (output labels) that will be loaded into this Notebook, along with a python dictionary that describes the X and Y data sets. \n",
    "\n",
    "#### Keras and TensorFlow\n",
    "Keras is a deep learning API written in Python that runs on top of the machine learning platform TensorFlow. It allows users to quickly and easily build a model with layers as building blocks. I will be using Keras layers to build up a CNN, and perhaps a CRNN after that depending on the success (or expected lack of success) of the CNN. \n",
    "\n",
    "#### Considerations\n",
    "This project falls under the \"multi-label\" classification model. This means that each example can be *multiple* classes of the available final classification options. Specifically, a bass drum event is independent from a snare drum event and from a hihat event and from a cymbal event and from a tom event, because a drummer, having multiple limbs, is able to simultaneously play multiple of these drum pieces. As such, the one-hot matrix Y can have multiple 1s in any given example row. \n",
    "\n",
    "Because of this, the final dense layer in the model **should not** be using the softmax ativation function. The softmax function chooses exactly one class and it is weighted by the presence of the other classes. Thus the activation function for the final layer needs to be **sigmoid function** because it properly projects the input to a probability between 0 and 1 without any consideration of or weighting from the other classes (using activation = 'sigmoid'). Additionally, the loss function used when the model is compiled should be loss='binary_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# get the X, Y, and note_dict data from the 25 song training set\\nfp = \"C:\\\\Users\\\\Thomas\\\\Python Projects\\\\Drum-Tabber-Support-Data\\\\Saved-Output\\\\\"\\nX_temp = np.load(fp + \\'X_june29th.npy\\')\\nY_all = np.load(fp + \\'Y_june29th.npy\\')\\nwith open(fp + \\'note_dict_june29th.json\\') as json_file: \\n    note_dict = json.load(json_file)\\nX_all = np.transpose(X_temp, (1,0,2))   # because of the way I had previously output the numpy array, should put the num_examples first\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import relevant packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "# get the X, Y, and note_dict data from the 25 song training set\n",
    "fp = \"C:\\\\Users\\\\Thomas\\\\Python Projects\\\\Drum-Tabber-Support-Data\\\\Saved-Output\\\\\"\n",
    "X_temp = np.load(fp + 'X_june29th.npy')\n",
    "Y_all = np.load(fp + 'Y_june29th.npy')\n",
    "with open(fp + 'note_dict_june29th.json') as json_file: \n",
    "    note_dict = json.load(json_file)\n",
    "X_all = np.transpose(X_temp, (1,0,2))   # because of the way I had previously output the numpy array, should put the num_examples first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_all.shape (199568, 200, 3)\n",
      "Y_all.shape (199568, 7, 3)\n",
      "['n_mels', 'sr', 'n_spectro_slices', 'include_LR', 'include_differential', 'X song order', 'X song length', 'X song index range', 'X spectro index order', 'Y column index order']\n",
      "{'ancient_tombs': [0, 11523], 'best_of_me': [11524, 18031], 'boulevard_of_broken_dreams': [18032, 23787], 'cant_be_saved': [23788, 32223], 'face_down': [32224, 41459], 'family_tradition': [41460, 51247], 'fireworks_at_dawn': [51248, 53675], 'forever_at_last': [53676, 60819], 'four_years': [60820, 69351], 'garden_state': [69352, 79979], 'gunpowder': [79980, 92575], 'hair_of_the_dog': [92576, 100187], 'lungs_like_gallows': [100188, 108071], 'misery_business': [108072, 117667], 'mookies_last_christmas': [117668, 125407], 'planning_a_prison_break': [125408, 140571], 'rollercoaster': [140572, 147871], 'sow': [147872, 153779], 'sugar_were_going_down': [153780, 158263], 'surprise_surprise': [158264, 165747], 'thats_what_you_get': [165748, 173135], 'the_dark': [173136, 183179], 'the_kill': [183180, 188311], 'the_rapture': [188312, 191635], 'wolves_at_the_door': [191636, 199567]}\n",
      "3\n",
      "['cant_be_saved', 'mookies_last_christmas', 'rollercoaster', 'thats_what_you_get']\n",
      "['garden_state', 'gunpowder', 'sugar_were_going_down', 'surprise_surprise']\n",
      "[['cant_be_saved', 'mookies_last_christmas', 'rollercoaster', 'thats_what_you_get'], ['garden_state', 'gunpowder', 'sugar_were_going_down', 'surprise_surprise']]\n",
      "[23788, 32223]\n",
      "(8435, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_all.shape\", X_all.shape)\n",
    "print(\"Y_all.shape\", Y_all.shape)\n",
    "print(list(note_dict.keys()))\n",
    "print(note_dict['X song index range'])\n",
    "index_range_d = note_dict['X song index range']\n",
    "print(int(0.15 * len(note_dict['X song index range'].keys())))\n",
    "random_sample = sorted(random.sample(list(index_range_d.keys()), k = 4))\n",
    "random_sample2 = sorted(random.sample([x for x in list(index_range_d.keys()) if x not in random_sample], k = 4))\n",
    "print(random_sample)\n",
    "print(random_sample2)\n",
    "combined = [random_sample, random_sample2]\n",
    "print(combined)\n",
    "                         \n",
    "print(index_range_d[random_sample[0]])\n",
    "print(X_all[index_range_d[random_sample[0]][0]:index_range_d[random_sample[0]][1]][:][:].shape)\n",
    "\n",
    "Xtest, Ytest, Xdtest, Ydtest, Xttest, Yttest = split_XY(X_all, Y_all, note_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144312, 200, 3) (144312, 7, 3) (27828, 200, 3) (27828, 7, 3) (27428, 200, 3) (27428, 7, 3)\n",
      "Xtest.shape (144312, 200, 3)\n",
      "Xtest_T.shape (144312, 3, 200)\n",
      "Xtest_T_reshaped.shape (432936, 200)\n"
     ]
    }
   ],
   "source": [
    "print(Xtest.shape, Ytest.shape, Xdtest.shape, Ydtest.shape, Xttest.shape, Yttest.shape)\n",
    "print(\"Xtest.shape\", Xtest.shape)\n",
    "Xtest_T = np.transpose(Xtest, (0,2,1))\n",
    "print(\"Xtest_T.shape\", Xtest_T.shape)\n",
    "ex = 2345\n",
    "\n",
    "\n",
    "Xd1 , Xd2, Xd3 = Xtest_T.shape\n",
    "Xtest_T_reshaped = np.reshape(Xtest_T, (Xd1*Xd2, Xd3))\n",
    "print(\"Xtest_T_reshaped.shape\", Xtest_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'tk_beat', '1': 'tk_downbeat', '2': 'BD_o', '3': 'SD_o', '4': 'HH_x', '5': 'at_o', '6': 'ac_x'}\n",
      "Ytest shape (144312, 7, 3)\n",
      "Ytest first example all \n",
      " [[1 1 1]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "First example, all labels, mono channel, from Ytest [1 0 0 1 0 1 0]\n",
      "17th example, all labels, left channel, from Ytest [1 1 1 0 0 0 1]\n",
      "Ytest_re shape (432936, 7)\n",
      "Ytest_re first example all \n",
      " [1 1 0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(note_dict['Y column index order'])\n",
    "print(\"Ytest shape\", Ytest.shape)\n",
    "Yd1, Yd2, Yd3 = Ytest.shape\n",
    "Ytest_re = np.reshape(Ytest, (-1, Yd2), order='F')\n",
    "\n",
    "\n",
    "print(\"Ytest first example all \\n\", Ytest[0])\n",
    "print(\"First example, all labels, mono channel, from Ytest\", Ytest[0,:,0])\n",
    "print(\"17th example, all labels, left channel, from Ytest\", Ytest[16,:,1])\n",
    "\n",
    "print(\"Ytest_re shape\", Ytest_re.shape)\n",
    "print(\"Ytest_re first example all \\n\", Ytest_re[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10  20  30  40  50  60  70  80  90]\n",
      " [ 20  40  60  80 100 120 140 160 180]\n",
      " [ 30  60  90 120 150 180 210 240 270]\n",
      " [ 40  80 120 160 200 240 280 320 360]\n",
      " [ 50 100 150 200 250 300 350 400 450]] (5, 9)\n",
      "[[ 11  21  31  41  51  61  71  81  91]\n",
      " [ 21  41  61  81 101 121 141 161 181]\n",
      " [ 31  61  91 121 151 181 211 241 271]\n",
      " [ 41  81 121 161 201 241 281 321 361]\n",
      " [ 51 101 151 201 251 301 351 401 451]] (5, 9)\n",
      "[[ 12  22  32  42  52  62  72  82  92]\n",
      " [ 22  42  62  82 102 122 142 162 182]\n",
      " [ 32  62  92 122 152 182 212 242 272]\n",
      " [ 42  82 122 162 202 242 282 322 362]\n",
      " [ 52 102 152 202 252 302 352 402 452]] (5, 9)\n",
      "xymat_concat\n",
      "[[ 10  20  30  40  50  60  70  80  90]\n",
      " [ 20  40  60  80 100 120 140 160 180]\n",
      " [ 30  60  90 120 150 180 210 240 270]\n",
      " [ 40  80 120 160 200 240 280 320 360]\n",
      " [ 50 100 150 200 250 300 350 400 450]\n",
      " [ 11  21  31  41  51  61  71  81  91]\n",
      " [ 21  41  61  81 101 121 141 161 181]\n",
      " [ 31  61  91 121 151 181 211 241 271]\n",
      " [ 41  81 121 161 201 241 281 321 361]\n",
      " [ 51 101 151 201 251 301 351 401 451]\n",
      " [ 12  22  32  42  52  62  72  82  92]\n",
      " [ 22  42  62  82 102 122 142 162 182]\n",
      " [ 32  62  92 122 152 182 212 242 272]\n",
      " [ 42  82 122 162 202 242 282 322 362]\n",
      " [ 52 102 152 202 252 302 352 402 452]] (15, 9)\n",
      "xymat_stacked\n",
      "[[[ 10  20  30  40  50  60  70  80  90]\n",
      "  [ 20  40  60  80 100 120 140 160 180]\n",
      "  [ 30  60  90 120 150 180 210 240 270]\n",
      "  [ 40  80 120 160 200 240 280 320 360]\n",
      "  [ 50 100 150 200 250 300 350 400 450]]\n",
      "\n",
      " [[ 11  21  31  41  51  61  71  81  91]\n",
      "  [ 21  41  61  81 101 121 141 161 181]\n",
      "  [ 31  61  91 121 151 181 211 241 271]\n",
      "  [ 41  81 121 161 201 241 281 321 361]\n",
      "  [ 51 101 151 201 251 301 351 401 451]]\n",
      "\n",
      " [[ 12  22  32  42  52  62  72  82  92]\n",
      "  [ 22  42  62  82 102 122 142 162 182]\n",
      "  [ 32  62  92 122 152 182 212 242 272]\n",
      "  [ 42  82 122 162 202 242 282 322 362]\n",
      "  [ 52 102 152 202 252 302 352 402 452]]] (3, 5, 9)\n"
     ]
    }
   ],
   "source": [
    "xdim = list(range(1,6))\n",
    "ydim = list(range(10,100,10))\n",
    "\n",
    "xymat = np.outer(xdim, ydim)\n",
    "print(xymat, xymat.shape)\n",
    "xymat_2 = xymat+1\n",
    "print(xymat_2, xymat_2.shape)\n",
    "xymat_3 = xymat+2\n",
    "print(xymat_3, xymat_3.shape)\n",
    "\n",
    "xymat_concat = np.concatenate([xymat, xymat_2, xymat_3], axis = 0)\n",
    "xymat_stacked = np.stack([xymat, xymat_2, xymat_3], axis = 0)\n",
    "print(\"xymat_concat\")\n",
    "print(xymat_concat, xymat_concat.shape)\n",
    "print(\"xymat_stacked\")\n",
    "print(xymat_stacked, xymat_stacked.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split_XY function\n",
    "Although there are probably TensorFlow functions that automatically do the following, I am going to write a function that splits the X and Y data sets into training, development (dev) and tests sets. I am doing this because I am attempting to preserve the order of the examples, since they are still in chronological order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_XY(X,Y, note_dict, dev_split = 0.15, test_split = 0.15, sample_each_song = False):\n",
    "    \"\"\"\n",
    "    Splits the full X and Y data set into a train, dev, and test split, based normally on the choice of random SONGS,\n",
    "    unless the sample each song boolean is true. If true, the data set is split in such a way that it takes a sequential portion\n",
    "    of each song, where the total portions are roughly each equal to the dev and test splits.\n",
    "    \n",
    "    Args:\n",
    "        X [np.array]: full X set from the create_XY function (n_examples, n_mels_total, n_spectro_channels)\n",
    "        Y [np.array]: full Y set from the create_XY function (n_examples, n_mels_total, n_spectro_channels)\n",
    "        note_dict [dict]: dictionary created in create_XY used to describe the \n",
    "        dev_split [float]: fraction of the full X, Y set to put in the dev set\n",
    "        test_split [float]: fraction of the full X, Y set to put in the test set (will not overlap with dev set)\n",
    "        sample_each_song [bool]: Default = False. If False, will separate the dev and test by FULL SONG only. \n",
    "                                                  If True, will take a continuous duration sample from each song in the training set, based on the dev and test split fractions\n",
    "        \n",
    "    Returns:\n",
    "        np.array: X_train\n",
    "        np.array: Y_train\n",
    "        np.array: X_dev\n",
    "        np.array: Y_dev\n",
    "        np.array: X_test\n",
    "        np.array: Y_test\n",
    "    \"\"\"\n",
    "    \n",
    "    def unstack_channels(matrix):\n",
    "        \"\"\"\n",
    "        Helper function used to undo the spectro channel stacking and \n",
    "        to place them in one large stack (treating them as individual examples)\n",
    "        \n",
    "        Returns:\n",
    "            np.array: a 2 dimensional numpy array, either unstacked (in the include LR channels case) or simply not a 3D numpy array anymore (in the mono only case)\n",
    "        \"\"\"\n",
    "        \n",
    "    song_index_dict = note_dict['X song index range']  # gets the dict where { song_title : index_range_of_that_songs_examples ([index_start, index_end]) }\n",
    "    songs = list(song_index_dict.keys())     # list of song title strings in entire data set\n",
    "    n_songs = len(songs)     # number of songs in the entire data set\n",
    "    \n",
    "    if not sample_each_song: # in the case where each song is NOT sampled, and instead entire songs are chosen for the dev and test sets\n",
    "        n_songs_dev = int(dev_split*n_songs)  # int number of songs in dev set\n",
    "        n_songs_test = int(test_split*n_songs)  # int number of songs in test set\n",
    "        songs_dev = sorted(random.sample(songs, k = n_songs_dev))    # grab dev number of songs from the songs list, and sorts alphabetically\n",
    "        songs_test = sorted(random.sample([x for x in songs if x not in songs_dev], k = n_songs_test))  # grab test number of songs from the songs list, but not including the dev list, and sorts alphabetically\n",
    "        \n",
    "        set_list = [songs_dev, songs_test]  # ha, set_list, like a band would play live a bunch of songs\n",
    "        \n",
    "        XdYdXtYt = [] # list of np.arrays that will correspond to, in order, X_dev, Y_dev, X_test, Y_test\n",
    "        index_range_list = [] # to be \n",
    "        # get slices of X,Y that correspond to the dev set first time through, test set second time through\n",
    "        for song_list in set_list:\n",
    "            X_dt_list = []   # will be a list of np.arrays from X\n",
    "            Y_dt_list = []   # will be a list of np.arrays from Y\n",
    "            for song in song_list:\n",
    "                index_range = song_index_dict[song]\n",
    "                index_range_list.append(index_range)   # keep all the index ranges [start, end_inclusive] that are used in dev and test set\n",
    "                X_song = X[ index_range[0]:index_range[1]+1 ][:][:]  # add 1 because the saved indices are INCLUSIVE and Python uses exclusive slicing\n",
    "                Y_song = Y[ index_range[0]:index_range[1]+1 ][:][:]\n",
    "                X_dt_list.append(X_song)\n",
    "                Y_dt_list.append(Y_song)\n",
    "            X_dt = np.concatenate(X_dt_list, axis = 0)\n",
    "            Y_dt = np.concatenate(Y_dt_list, axis = 0)\n",
    "            XdYdXtYt.append(X_dt)\n",
    "            XdYdXtYt.append(Y_dt)\n",
    "\n",
    "        # set the outputs equal to their proper sets from the list\n",
    "        X_dev, Y_dev, X_test, Y_test = XdYdXtYt[0], XdYdXtYt[1], XdYdXtYt[2], XdYdXtYt[3]\n",
    "        \n",
    "        # delete the indexes of the used songs from the X and Y full sets to create the X and Y training sets\n",
    "        all_index = []\n",
    "        for obj in index_range_list:\n",
    "            all_index = all_index + list(range(obj[0], obj[1]+1))\n",
    "        X_train = np.delete(X, all_index, axis=0)\n",
    "        Y_train = np.delete(Y, all_index, axis=0)\n",
    "    \n",
    "    else: # in the case where each song IS sampled roughly equally according to the dev_split and test_split, still in chrono order in the songs and alphabetical by song title\n",
    "        None\n",
    "    \n",
    "    return X_train, Y_train, X_dev, Y_dev, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_2018(n_classes, n_mels, n_spectro_context, include_differential):\n",
    "    \"\"\"\n",
    "    Creates a Keras NN model for the processing of the data. \n",
    "    Note that this NN model is based on the \"Towards Multi-Instrument Drum Transcription\" paper's model\n",
    "    \n",
    "    Args:\n",
    "        n_classes [int]: the total number of classes output by the model, as dictated by the the second dimension of the Y matrix\n",
    "        n_mels [int]: the total number of mels used to create the spectrogram slices. Decided by the note_dict (made in create_XY function)\n",
    "        n_spectro_context [int]: (should be greater than 3)\n",
    "        include_differential [bool]: Did the data set include the first time differential. Decided by the note_dict\n",
    "        \n",
    "    Returns:\n",
    "        Keras Model:\n",
    "    \"\"\"\n",
    "    \n",
    "    n_features = n_mels\n",
    "    if include_differential:        \n",
    "        n_features = n_features * 2   # double the number of features if first time differential was included\n",
    "    input_shape = (n_features, n_spectro_context, 1)  # to use the Conv2D later on we need to say there is only 1 channel in the \n",
    "    \n",
    "    x = Input(shape = input_shape, name = 'input', dtype = 'float32')  # returns a TensorFlow symbolic tensor object\n",
    "    \n",
    "    # normalization of the initial input data\n",
    "    y = BatchNormalization(axis=1)(x)  # batch norm is normally done over the feature axis. Current shape here is: (None, features, context (time axis), channel=1)\n",
    "    \n",
    "    # 2 x convolutional layer (32 filter x  3x3)\n",
    "    y = Conv2D(32, (3,3), padding = 'same')(y)   # Conv2D  (num_filters, (kernal_size_dim1, kernal_size_dim2), strides = (1,1), padding = 'valid', ) 'Valid' padding has NO padding, so \"image\" will shrink. \"Same\" padding HAS padding to preserve the dimensions of the \"image\"\n",
    "    y = Activation('relu')(y)                    # activation layer \n",
    "    y = BatchNormalization()(y)\n",
    "    y = Conv2D(32, (3,3), padding = 'same')(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    \n",
    "    # max pool (1x3)\n",
    "    y = MaxPool2D(pool_size = (1,3))(y)\n",
    "    \n",
    "    # 2 x convolutional layer (64 filter x  3x3)\n",
    "    y = Conv2D(64, (3,3), padding = 'same')(y)   # Conv2D  (num_filters, (kernal_size_dim1, kernal_size_dim2), strides = (1,1), padding = 'valid', ) 'Valid' padding has NO padding, so \"image\" will shrink. \"Same\" padding HAS padding to preserve the dimensions of the \"image\"\n",
    "    y = Activation('relu')(y)                    # activation layer \n",
    "    y = BatchNormalization()(y)\n",
    "    y = Conv2D(64, (3,3), padding = 'same')(y)\n",
    "    y = Activation('relu')(y)                    # activation layer \n",
    "    y = BatchNormalization()(y)\n",
    "    \n",
    "    \n",
    "     # max pool (1x3)\n",
    "    y = MaxPool2D(pool_size = (1,3))(y)\n",
    "    \n",
    "    # 2 x dense (256)\n",
    "    y = Flatten()(y)\n",
    "    y = Dense(256)(y)\n",
    "    y = Activation('relu')(y)   \n",
    "    y = Dense(256)(y)\n",
    "    y = Activation('relu')(y)   \n",
    "    y = Dense(n_classes, activation = 'sigmoid')(y)\n",
    "    \n",
    "    return keras.Model(inputs = x, outputs = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 200, 32, 1)]      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_80 (Batc (None, 200, 32, 1)        800       \n",
      "_________________________________________________________________\n",
      "conv2d_61 (Conv2D)           (None, 200, 32, 32)       320       \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 200, 32, 32)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_81 (Batc (None, 200, 32, 32)       128       \n",
      "_________________________________________________________________\n",
      "conv2d_62 (Conv2D)           (None, 200, 32, 32)       9248      \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 200, 32, 32)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_82 (Batc (None, 200, 32, 32)       128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 200, 10, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_63 (Conv2D)           (None, 200, 10, 64)       18496     \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 200, 10, 64)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_83 (Batc (None, 200, 10, 64)       256       \n",
      "_________________________________________________________________\n",
      "conv2d_64 (Conv2D)           (None, 200, 10, 64)       36928     \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 200, 10, 64)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_84 (Batc (None, 200, 10, 64)       256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 200, 3, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 38400)             0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 256)               9830656   \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 7)                 1799      \n",
      "=================================================================\n",
      "Total params: 9,964,807\n",
      "Trainable params: 9,964,023\n",
      "Non-trainable params: 784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "testmodel = CNN_2018(7, 100, 32, True)\n",
    "testmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "testmodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of Useful Shortcuts\n",
    "\n",
    "* Ctrl + shift + P = List of Shortcuts\n",
    "* Enter (command mode) = Enter Edit Mode (enter cell to edit it)\n",
    "* Esc (edit mode) = Enter Command Mode (exit cell)\n",
    "* A = Create Cell above\n",
    "* B = Create Cell below\n",
    "* D,D = Delete Cell\n",
    "* Shift + Enter = Run Cell (code or markdown)\n",
    "* M = Change Cell to Markdown\n",
    "* Y = Change Cell to Code\n",
    "* Ctrl + Shift + Minus = Split Cell at Cursor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
