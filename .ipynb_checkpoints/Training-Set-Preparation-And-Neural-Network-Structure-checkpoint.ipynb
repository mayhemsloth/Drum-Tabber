{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Set Preparation and Neural Network Structure\n",
    "This Jupyter Notebook is the product of Thomas Hymel. The contents pertain to one part of a Automatic Drum Transcription (ADT) project that I am working on to improve data science and machine learning skills. This notebook in particular focuses on the ADT feature extraction typically used in ADT and audio processing.\n",
    "\n",
    "### Audio Data Pre-processing and Explanation\n",
    "There are many models used in ADT but **I am most interested in using convolutional neural networks (CNNs) and perhaps convolutional recurrent neural networks (CRNNs)** as they seem to perform the best in the most recent literature. For CNNs and CRNNs almost always the first step in preprocessing the audio so that features can be properly extracted by the NN is **creating a log mel-spectrogram**. This [blog post here](https://towardsdatascience.com/getting-to-know-the-mel-spectrogram-31bca3e2d9d0) explains well what a log mel-spectrogram is, but I will summarize it now, from the beginning of sound, so that I can internally understand it better from its base parts. \n",
    "\n",
    "Sound is simply a longitudinal pressure wave, transmitted through vibrations in a medium (usually air). That pressure wave can be measured and characterized as an amplitude as a function of time. Because computer electronics are based around digital objects, a continuous wave (like sound) must be **sampled** to represent it as a discrete sequence of amplitude numbers and store it as a digital object. The *rate* at which a continuous wave is sampled will affect the eventual apparent output shape of that wave. So what *rate* is chosen to sample a wave to make it discrete? Because humans have evolved to hear sound in air for the frequencies between 20 Hz and ~20,000 Hz, we can assume that we don't care about any frequencies above 20,000 Hz. This is our [Nyquist frequency](https://en.wikipedia.org/wiki/Nyquist_frequency), so we know we need to sample at >40,000 Hz in order to safely capture all the *frequency* information needed in a discretized sound wave to faithfully recreate it back to a continuous sound wave without humans detecting a difference. The standard audio sampling rate used pretty much everywhere is indeed **44,100 Hz**. Furthermore, in order to properly capture the dynamics of a small and large amplitudes of these pressure waves, the *bit depth* is also often defined (that is, how many different integer numbers you are allowed to use to map the continuous amplitude). The bit depth is pretty much standard at 16 bits (CD quality) or 24 bits (DVD/BluRay audio). \n",
    "\n",
    "Digital audio is thus an ordered 1D array of integers, that can, with the properly sampling rate, be converted back to a \"continuous\" wave and sound through speakers. However, **an ordered 1D array of integers is usually *not* the thing that is fed into a NN directly.** [Note that this case is different from computer vision, where the digital representation of images are often directly fed into NNs (rather than their k-space interpretation).] Any continuous wave object displayed in the time-domain can be transformed using **Fourier series transformations** to have it be represented in the frequency domain, which displays the relative prevelance of the different frequencies within that wave object. The same could be said for discrete wave objects and an appropriately changed Fourier transform function. For ADT and other music information retrieval tasks, the Fourier transform of an *entire song* at once is not that useful, because it doesn't describe the drum events or notes played as a function of time (just as a function of the entire song). If the goal was to know which frequencies occurred in some small time frame, you could slice the song up into tiny frames and *then* do a Fourier transform on *each* of these frames. Indeed, that is exactly what occurs with a [short-time Fourier transform](https://en.wikipedia.org/wiki/Short-time_Fourier_transform) (STFT). Remember that we are working under the assumption that drum events are \"instantaneous\" events. Slicing up a song into tiny durations of time, and *then* taking the Fourier transforms of those tiny slices is a reasonable approach to attempting to find \"instantaneous\" events. In reality, drum events do have some characteristic \"duration\", so ensuring that your transform slices aren't too *small* for these durations is an important consideration.  \n",
    "\n",
    "So you now have a representation of the audio, as a function of discrete time steps, for which frequencies occur at (roughly) that specific time. **This representation is called a [spectrogram](https://en.wikipedia.org/wiki/Spectrogram)**, defined as a visual representation of a spectrum of frequencies of a signal as the signal varies with time. On the x-axis is time, the y-axis is the frequencies (in Hz), and then each point on that graph/matrix has a value, and is colored a certain color depending on the prevelance of that frequency, to produce a heat map. A **log spectrogram** is simply representing the y-axis as a log axis (to more accurately represent how humans hear) *and* the color axis on a log axis (to more accurately represent decibels, dB, which is also how humans hear). So what is a **log *mel*-spectrogram?** Well, to *even further* accurately represent how humans hear sound, we can transform the frequency axis (in Hz) into a different unit called mels (short for melody?). The [mel scale](https://en.wikipedia.org/wiki/Mel_scale) is \"a perceptual scale of pitches judged by listeners to be equal in distance from one another\". This mel scale is effectively a non-linear frequency axis transformation that takes the Hz frequencies and puts them into \"equally spaced\" bins that are \"equal in distance\" according to how humans hear them. Once the y-axis has been transformed from Hz to mels, then you are left with a log mel-spectrogram of some audio clip. As a data structure, a mel-spectrogram is a 2D array, where the dimensions are determined by the length of the audio clip (in seconds), the hop length (number of samples between successive frames), the window length (how many samples for each STFT), and the number of mel bins.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log Mel-Spectrogram Exploration\n",
    "In order to utilize the log mel-spectrogram, a Python package will by imported for the purpose. Here we will use libROSA, which is for music and audio analysis and provides the building blocks necessary to create music information retrieval systems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing important packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pydub import AudioSegment   # main class from pydub package used to upload mp3 into Python and then get a NumPy array\n",
    "import IPython.display as ipd    # ability to play audio in Jupyter Notebooks if needed\n",
    "\n",
    "# importing newly relevant packages for log mel-spectrogram exploration\n",
    "import matplotlib as mpl\n",
    "import librosa as lb\n",
    "import librosa.display\n",
    "\n",
    "test_file = 'may3beta.mp3'\n",
    "samples_mono, sr = lb.load(test_file, sr=None, mono=True)\n",
    "\n",
    "\n",
    "# exports the raw data into an array.array object with the format [sample_1_L, sample_1_R, sample_2_L, sample_2_R, ...]\n",
    "# test_song = AudioSegment.from_mp3(test_file)\n",
    "# samples = np.array(test_song.get_array_of_samples()).reshape((-1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = lb.feature.melspectrogram(samples_mono, sr=sr, hop_length = 512, n_mels = 250)\n",
    "S_DB = lb.power_to_db(S, ref=np.max)\n",
    "librosa.display.specshow(S_DB, sr=sr, x_axis='time', y_axis = 'mel')\n",
    "print(S.shape)\n",
    "print(4*len(samples_mono)/2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making log mel-spectrograms from MAT dataframes and target labeling considerations\n",
    "The point of the music-aligned tab dataframes is that I can easily provide a drum piece label to a slice of music, in the hopes that the music/label pairing could potentially be used as an example in a training, development, and/or test set. Well, let's think more deeply about this potential setup, and compare it to previous papers or data sets that utilize spectrograms and CNNs or CRNNs for model building. \n",
    "\n",
    "Let's say we already have a model that \"works\" in the ideal case: that the model takes in a song (and maybe its BPM or other simple information about the song) and outputs a drum tab (either with the measure character or not). In order to properly place the onset of a drum event, the model would need to check sufficiently small slices of the song, just as the training data had fed the model sufficiently small slices of the the training songs. **However there's a potential problem here and it has to do with the \"phase\" associated with tabbing a song and slicing it the way that I have done.** Currently, given some single 16th note grid event, the song slice associated with that 16th note event starts at the \"start\" of that 16th note (as determined by the human assigning the first drum note onset) and ends at the beginning of the next 16th note event. This establishes some \"phase\" of the tab. In fact, an equally valid way to slice the song and assign the drum event labels could be *some amount* of samples starting forward in time, instead of at the \"start\" of the 16th note grid event, up to half the length of the 16th note resolution. A better way to think about this is to think about what happens when a song gets put into an ideal model. If you slice the song at the wrong \"phase\" you may never encounter *any* drum event, as determined by the model, because the SFTFs that make up the spectrogram will consistently look *different* to the model than those slices that the model was trained on. The solution to this problem is to make the hop size so small that no matter where the slicing of the song *starts* (to be transformed into STFTs and spectrograms) that a certain slice *will* be captured to appropriately be interpreted by the model as having a drum note onset in it. \n",
    "\n",
    "Indeed, this is the case that previous papers describe. For example, in \"Drum Transcription via Joint Beat and Drum Modeling Using Convolutional Recurrent Neural Networks\" by Vogl et. al., a paper which uses log spectrograms to train RNNs, CNNs, and CRNNs, the window size in the STFTs was 2048 samples, or **46.4 ms** long, and they did this STFT every **10 ms** which is the frame size, and then assigned a drum note onset to one single frame. Checking a song every 10 ms will certainly find a window that entirely contains any potential drum onset in the song. In comparison, for a 150 BPM song, the 16th note grid resolution is 100 ms. Checking every 10 ms, in my opinion, is potentially overkill, but I guess during training and testing I can see what changing that hyperparameter does to the performance of the model. \n",
    "\n",
    "So it seems like for training NNs using spectrograms, the target functions are generated by setting frames of a signal with the same frame rate as the input features to the correct class label. Basically you are assigning each hop size (each frame) to a specific label. This practice makes perfect sense to me. The main decision that you need to make is **how many frames do you assign as being \"correct\"**, and **how long are your window sizes**, and do those two decisions interact with each other? \n",
    "\n",
    "I think that I want to do the following: I want to have a song go into the model accompanied by its BPM. (Alternatively, you could use a librosa built-in function to approximate the BPM becasue the exactness doesn't really matter). In this way you can make the STFT window that makes up the spectrogram dependent on the BPM, while the total frame number will *not* be a function of the BPM. For example, if you give it two songs that are each 4 minutes, but one has BPM of 120 and another has BPM of 160, the total number of frames that it will process will be the same because that is only dependent on the hop size and the total duration. However the STFT will yield be different because the window sizes will be different. The point of this is to reflect how the labels are produced: the information of the labels is gathered only based on a BPM basis. That is, the window sizes of the training data itself will be different.\n",
    "\n",
    "Let's say a song's BPM is 150, so the 16th note resolution would be 15000/150 = 100 ms, which is my current plan for the calculation of the STFT window size. Let's say we have a slice that has some drum event in it. If a STFT has a length of 100 ms, and then we have a hop size of 20 ms (882 samples), that means that *some* part of any one 16th note slice of the song would be contained in (100 ms / 20 ms)x2 = ~10 STFTs. For example, if our 16th note grid slice starts at 350 ms and ends at 450 ms, and our hops are 20 ms along the 20 ms intervals, then the *first* time that a STFT would contain any part of the 16th grid drum event would be at the 260 ms-360 ms STFT slice, with a true overlap ratio of 10% = (360-350)/100. After that, some part of the 16th note drum event would be found in the STFT slices starting at 280 (30%), 300 (50%), 320 (70%), 340 (90%), 360 (90%), 380 (70%), 400 (50%), 420 (30%), and 440 (10%). If this song is being used as a training sample, which of these STFT slices *should* be assigned to a drum event label, and which should not be? The drum events are most likely to be at the beginning of the 16th note duration, although in reality the entirety of the 16th note duration is \"useful\" to identify the drum event. I probaly will decide that any STFT slice that overlaps some percentage (for example, at least 80%) of the drum note event slice would count as a positive in the label, but this is probably something that I can tinker around with later on. Alternatively, since I am assuming that I am slicing the song at the *beginning* of the drum notes, I could include the one that is closest to the starting time, and a a couple hops before that as well. This method would ensure that for any positive label in the STFT slices, *at least* the beginning of a 16th note slice would be included in that STFT slice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of Useful Shortcuts\n",
    "\n",
    "* Ctrl + shift + P = List of Shortcuts\n",
    "* Enter (command mode) = Enter Edit Mode (enter cell to edit it)\n",
    "* Esc (edit mode) = Enter Command Mode (exit cell)\n",
    "* A = Create Cell above\n",
    "* B = Create Cell below\n",
    "* D,D = Delete Cell\n",
    "* Shift + Enter = Run Cell (code or markdown)\n",
    "* M = Change Cell to Markdown\n",
    "* Y = Change Cell to Code\n",
    "* Ctrl + Shift + Minus = Split Cell at Cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
