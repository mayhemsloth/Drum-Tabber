{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Neural Network Model Building\n",
    "This Jupyter Notebook is the product of Thomas Hymel. The contents pertain to one part of a Automatic Drum Transcription (ADT) project that I am working on to improve my data science and machine learning skills. This notebook in particular focuses on the neural network model building using Keras and TensorFlow. The data used for training the model will initially be the 25 song data set that has been created in other Jupyter Notebooks. The training data is in two matrices X (input) and Y (output labels) that will be loaded into this Notebook, along with a python dictionary that describes the X and Y data sets. \n",
    "\n",
    "#### Keras and TensorFlow\n",
    "Keras is a deep learning API written in Python that runs on top of the machine learning platform TensorFlow. It allows users to quickly and easily build a model with layers as building blocks. I will be using Keras layers to build up a CNN, and perhaps a CRNN after that depending on the success (or expected lack of success) of the CNN. \n",
    "\n",
    "#### Considerations\n",
    "This project falls under the \"multi-label\" classification model. This means that each example can be *multiple* classes of the available final classification options. Specifically, a bass drum event is independent from a snare drum event and from a hihat event and from a cymbal event and from a tom event, because a drummer, having multiple limbs, is able to simultaneously play multiple of these drum pieces. As such, the one-hot matrix Y can have multiple 1s in any given example row. \n",
    "\n",
    "Because of this, the final dense layer in the model **should not** be using the softmax ativation function. The softmax function chooses exactly one class and it is weighted by the presence of the other classes. Thus the activation function for the final layer needs to be **sigmoid function** because it properly projects the input to a probability between 0 and 1 without any consideration of or weighting from the other classes (using activation = 'sigmoid'). Additionally, the loss function used when the model is compiled should be loss='binary_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# get the X, Y, and note_dict data from the 25 song training set\\nfp = \"C:\\\\Users\\\\Thomas\\\\Python Projects\\\\Drum-Tabber-Support-Data\\\\Saved-Output\\\\\"\\nX_temp = np.load(fp + \\'X_june29th.npy\\')\\nY_all = np.load(fp + \\'Y_june29th.npy\\')\\nwith open(fp + \\'note_dict_june29th.json\\') as json_file: \\n    note_dict = json.load(json_file)\\nX_all = np.transpose(X_temp, (1,0,2))   # because of the way I had previously output the numpy array, should put the num_examples first\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import relevant packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "# get the X, Y, and note_dict data from the 25 song training set\n",
    "fp = \"C:\\\\Users\\\\Thomas\\\\Python Projects\\\\Drum-Tabber-Support-Data\\\\Saved-Output\\\\\"\n",
    "X_temp = np.load(fp + 'X_june29th.npy')\n",
    "Y_all = np.load(fp + 'Y_june29th.npy')\n",
    "with open(fp + 'note_dict_june29th.json') as json_file: \n",
    "    note_dict = json.load(json_file)\n",
    "X_all = np.transpose(X_temp, (1,0,2))   # because of the way I had previously output the numpy array, should put the num_examples first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_all.shape (199568, 200, 3)\n",
      "Y_all.shape (199568, 7, 3)\n",
      "['n_mels', 'sr', 'n_spectro_slices', 'include_LR', 'include_differential', 'X song order', 'X song length', 'X song index range', 'X spectro index order', 'Y column index order']\n",
      "{'ancient_tombs': [0, 11523], 'best_of_me': [11524, 18031], 'boulevard_of_broken_dreams': [18032, 23787], 'cant_be_saved': [23788, 32223], 'face_down': [32224, 41459], 'family_tradition': [41460, 51247], 'fireworks_at_dawn': [51248, 53675], 'forever_at_last': [53676, 60819], 'four_years': [60820, 69351], 'garden_state': [69352, 79979], 'gunpowder': [79980, 92575], 'hair_of_the_dog': [92576, 100187], 'lungs_like_gallows': [100188, 108071], 'misery_business': [108072, 117667], 'mookies_last_christmas': [117668, 125407], 'planning_a_prison_break': [125408, 140571], 'rollercoaster': [140572, 147871], 'sow': [147872, 153779], 'sugar_were_going_down': [153780, 158263], 'surprise_surprise': [158264, 165747], 'thats_what_you_get': [165748, 173135], 'the_dark': [173136, 183179], 'the_kill': [183180, 188311], 'the_rapture': [188312, 191635], 'wolves_at_the_door': [191636, 199567]}\n",
      "3\n",
      "['gunpowder', 'mookies_last_christmas', 'sugar_were_going_down', 'wolves_at_the_door']\n",
      "['best_of_me', 'four_years', 'hair_of_the_dog', 'the_rapture']\n",
      "[['gunpowder', 'mookies_last_christmas', 'sugar_were_going_down', 'wolves_at_the_door'], ['best_of_me', 'four_years', 'hair_of_the_dog', 'the_rapture']]\n",
      "[79980, 92575]\n",
      "(12595, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_all.shape\", X_all.shape)\n",
    "print(\"Y_all.shape\", Y_all.shape)\n",
    "print(list(note_dict.keys()))\n",
    "print(note_dict['X song index range'])\n",
    "index_range_d = note_dict['X song index range']\n",
    "print(int(0.15 * len(note_dict['X song index range'].keys())))\n",
    "random_sample = sorted(random.sample(list(index_range_d.keys()), k = 4))\n",
    "random_sample2 = sorted(random.sample([x for x in list(index_range_d.keys()) if x not in random_sample], k = 4))\n",
    "print(random_sample)\n",
    "print(random_sample2)\n",
    "combined = [random_sample, random_sample2]\n",
    "print(combined)\n",
    "                         \n",
    "print(index_range_d[random_sample[0]])\n",
    "print(X_all[index_range_d[random_sample[0]][0]:index_range_d[random_sample[0]][1]][:][:].shape)\n",
    "\n",
    "Xtest, Ytest, Xdtest, Ydtest, Xttest, Yttest = split_XY(X_all, Y_all, note_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34745, 200, 3) (34745, 7, 3) (19273, 200, 3) (19273, 7, 3)\n"
     ]
    }
   ],
   "source": [
    "print(Xdtest.shape, Ydtest.shape, Xttest.shape, Yttest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split_XY function\n",
    "Although there are probably TensorFlow functions that automatically do the following, I am going to write a function that splits the X and Y data sets into training, development (dev) and tests sets. I am doing this because I am attempting to preserve the order of the examples, since they are still in chronological order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_XY(X,Y, note_dict, dev_split = 0.15, test_split = 0.15, sample_each_song = False):\n",
    "    \"\"\"\n",
    "    Splits the full X and Y data set into a train, dev, and test split, based normally on the choice of random SONGS,\n",
    "    unless the sample each song boolean is true. If true, the data set is split in such a way that it takes a sequential portion\n",
    "    of each song, where the total portions are roughly each equal to the dev and test splits.\n",
    "    \n",
    "    Args:\n",
    "        X [np.array]:\n",
    "        Y [np.array]:\n",
    "        note_dict [dict]: dictionary created in create_XY used to describe the \n",
    "        dev_split [float]:\n",
    "        test_split [float]:\n",
    "        sample_each_song [bool]:\n",
    "        \n",
    "    Returns:\n",
    "        np.array: X_train\n",
    "        np.array: Y_train\n",
    "        np.array: X_dev\n",
    "        np.array: Y_dev\n",
    "        np.array: X_test\n",
    "        np.array: Y_test\n",
    "    \"\"\"\n",
    "    \n",
    "    song_index_dict = note_dict['X song index range']  # gets the dict where { song_title : index_range_of_that_songs_examples ([index_start, index_end]) }\n",
    "    songs = list(song_index_dict.keys())     # list of song title strings in entire data set\n",
    "    n_songs = len(songs)     # number of songs in the entire data set\n",
    "    \n",
    "    if not sample_each_song: # in the case where each song is NOT sampled, and instead entire songs are chosen for the dev and test sets\n",
    "        n_songs_dev = int(dev_split*n_songs)  # int number of songs in dev set\n",
    "        n_songs_test = int(test_split*n_songs)  # int number of songs in test set\n",
    "        songs_dev = sorted(random.sample(songs, k = n_songs_dev))    # grab dev number of songs from the songs list, and sorts alphabetically\n",
    "        songs_test = sorted(random.sample([x for x in songs if x not in songs_dev], k = n_songs_test))  # grab test number of songs from the songs list, but not including the dev list, and sorts alphabetically\n",
    "        \n",
    "        set_list = [songs_dev, songs_test]  # ha, set_list, like a band would play live a bunch of songs\n",
    "        \n",
    "        XdYdXtYt = [] # list of np.arrays that will correspond to, in order, X_dev, Y_dev, X_test, Y_test\n",
    "        # get slices of X,Y that correspond to the dev set first time through, test set second time through\n",
    "        for song_list in set_list:\n",
    "            X_dt_list = []   # will be a list of np.arrays from X\n",
    "            Y_dt_list = []   # will be a list of np.arrays from Y\n",
    "            for song in song_list:\n",
    "                index_range = song_index_dict[song]\n",
    "                X_song = X[ index_range[0]:index_range[1] ][:][:]\n",
    "                Y_song = Y[ index_range[0]:index_range[1] ][:][:]\n",
    "                X_dt_list.append(X_song)\n",
    "                Y_dt_list.append(Y_song)\n",
    "            X_dt = np.concatenate(X_dt_list, axis = 0)\n",
    "            Y_dt = np.concatenate(Y_dt_list, axis = 0)\n",
    "            XdYdXtYt.append(X_dt)\n",
    "            XdYdXtYt.append(Y_dt)\n",
    "        \n",
    "        # set the outputs equal to their proper sets from the list\n",
    "        X_dev, Y_dev, X_test, Y_test = XdYdXtYt[0], XdYdXtYt[1], XdYdXtYt[2], XdYdXtYt[3]\n",
    "        \n",
    "    \n",
    "    else: # in the case where each song IS sampled roughly equally according to the dev_split and test_split, still in chrono order in the songs and alphabetical by song title\n",
    "        None\n",
    "    \n",
    "    # NEED TO REMOVE THE XY_DEV and XY_TEST LATER\n",
    "    X_train = X\n",
    "    Y_train = Y\n",
    "    \n",
    "    return X_train, Y_train, X_dev, Y_dev, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_2018(input_shape, n_classes, n_mels, include_differential, include_LR):\n",
    "    \"\"\"\n",
    "    Creates a Keras NN model for the processing of the data. \n",
    "    Note that this NN model is based on the \"Towards Multi-Instrument Drum Transcription\" paper's model\n",
    "    \n",
    "    Args:\n",
    "        input_shape\n",
    "        \n",
    "    Returns:\n",
    "        Keras Model:\n",
    "    \"\"\"\n",
    "    \n",
    "    x = Input(shape = input_shape, name = 'input', dtype = 'float32')\n",
    "    \n",
    "    # normalization of the initial input data\n",
    "    y = BatchNormalization(axis=2)(x)\n",
    "    \n",
    "    # 2 x convolutional layer (32 filter x  3x3)\n",
    "    y = Conv2D()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Conv2D()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    \n",
    "    # max pool (1x3)\n",
    "    y = MaxPool2D()(y)\n",
    "    \n",
    "    # 2 x convolutional layer (32 filter x  3x3)\n",
    "    \n",
    "     # max pool (1x3)\n",
    "    y = MaxPool2D()(y)\n",
    "    \n",
    "    # 2 x dense (256)\n",
    "    y = Flatten()(y)\n",
    "    y = Dense(64, activation = 'relu')(y)\n",
    "    \n",
    "    \n",
    "    return Model(inputs = x, outputs = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of Useful Shortcuts\n",
    "\n",
    "* Ctrl + shift + P = List of Shortcuts\n",
    "* Enter (command mode) = Enter Edit Mode (enter cell to edit it)\n",
    "* Esc (edit mode) = Enter Command Mode (exit cell)\n",
    "* A = Create Cell above\n",
    "* B = Create Cell below\n",
    "* D,D = Delete Cell\n",
    "* Shift + Enter = Run Cell (code or markdown)\n",
    "* M = Change Cell to Markdown\n",
    "* Y = Change Cell to Code\n",
    "* Ctrl + Shift + Minus = Split Cell at Cursor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
